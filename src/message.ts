import { type Content, text, toContent } from "./content";
import { stringify } from "./json";
import { type TypedSchema, applySchema } from "./schema";
import {
  type ChatMemory,
  type ChatMemoryPatch,
  ContentMemoryNonSerializablePatch,
  type Tool,
  type ToolCallResponse,
  type ToolHandler,
  type Tools
} from "./tool";

/**
 * Enum‑like string union for every recognized chat role.
 */
export type MessageRole =
  | "system"
  | "user"
  | "assistant"
  | "tool"
  | "function" // legacy OpenAI
  | "system_function" // internal use
  | "tool_response" // OpenAI variant
  | "assistant_function_call"; // transitional internal role

/** Base shape for every message variant. */
interface BaseMessage<R extends MessageRole> {
  role: R;
}

/** Message sent by the system or developer instructions. */
export interface SystemMessage extends BaseMessage<"system"> {
  content: Content;
}

/** Human user message. */
export interface UserMessage extends BaseMessage<"user"> {
  content: Content;
  /** Is this generated for an agent step? */
  fake?: boolean;
}

/** Assistant reply that may embed tool‑calls. */
export interface AssistantMessage extends BaseMessage<"assistant"> {
  content: Content | null;
  tool_calls?: ToolCall[];
}

/** Response generated by a tool handler. */
export interface ToolMessage extends BaseMessage<"tool"> {
  tool_call_id: string;
  content: Content;
}

/** Function (legacy OpenAI) message. */
export interface FunctionMessage extends BaseMessage<"function"> {
  name: string;
  content: Content;
}

/** Discriminated union covering every message kind used by the framework. */
export type Message =
  | SystemMessage
  | UserMessage
  | AssistantMessage
  | ToolMessage
  | FunctionMessage;

/** Convenience factory for a **user** message. */
export const UserMessage = (c: string | Content): UserMessage => ({
  role: "user",
  content: toContent(c)
});

/** Convenience factory for a **system** message. */
export const SystemMessage = (c: string | Content): SystemMessage => ({
  role: "system",
  content: toContent(c)
});

/** Convenience factory for an **assistant** message (optionally with tool‑calls). */
export const AssistantMessage = (
  c: string | Content | null,
  tool_calls?: ToolCall[]
): AssistantMessage => ({
  role: "assistant",
  content: toContent(c),
  ...(tool_calls ? { tool_calls } : {})
});

/** Schema for an individual tool call attached to an assistant turn. */
export type ToolCall = {
  id: string;
  type: "function";
  function: {
    name: string;
    arguments: string | Record<string, unknown>;
  };
};

/** Type‑guard detecting whether an assistant message contains tool calls. */
export const isAssistantToolCall = (
  msg: Message | undefined
): msg is AssistantMessage & { tool_calls: ToolCall[] } =>
  msg?.role === "assistant" &&
  Array.isArray(msg.tool_calls) &&
  msg.tool_calls.length > 0;

/** Parse and validate the `arguments` field of a {@link ToolCall}. */
export const getToolArguments = (
  input: string | Record<string, unknown>
): Record<string, unknown> => {
  if (typeof input === "string") {
    try {
      return JSON.parse(input);
    } catch (err) {
      const snippet = input.slice(0, 80) + (input.length > 80 ? "…" : "");
      throw new Error(`Failed to parse tool arguments string: ${snippet}`);
    }
  }
  return input;
};

/**
 * Normalize a {@link ToolCallResponse} into a single {@link Content} item.
 * Falls back to human‑readable text when the target provider cannot handle
 * structured payloads.
 */
const normalizeToolResponse = <Memory extends ChatMemory, Out>(
  response: ToolCallResponse<Memory, Out>,
  mode: "openai" | "mcp"
): Content => {
  if (response.error) return text(`Error: ${response.error}`);
  if (!response.content || response.content.length === 0)
    return text("Empty response.");
  // @todo First response only?
  // biome-ignore lint/style/noNonNullAssertion: type narrowed
  if (mode === "mcp") return response.content[0]!;

  // OpenAI fallback → flatten to string.
  const flat = response.content
    .map((c) => {
      if (c.type === "text") return c.text;
      if (c.type === "json") return stringify(c.data);
      if (c.type === "image") return `[Image: ${c.mimeType ?? "unknown"}]`;
      return "[Unsupported content type]";
    })
    .join("\n\n");
  return text(flat);
};

/** Execute a single {@link ToolCall}. */
export const executeToolCall = async <
  In extends Record<string, unknown>,
  Out,
  Memory extends ChatMemory
>(
  toolCall: ToolCall,
  handler: ToolHandler<In, Out, Memory>,
  schema: TypedSchema<In>,
  memory: Memory,
  mode: "openai" | "mcp" = "openai"
): Promise<{ message: ToolMessage; memPatch?: ChatMemoryPatch }> => {
  try {
    const parsedArgs = getToolArguments(toolCall.function.arguments);
    const checkedArgs = applySchema(schema, parsedArgs as Partial<In>);

    const {
      error,
      content,
      [ContentMemoryNonSerializablePatch]: memPatch
    } = await handler(checkedArgs, memory);

    return {
      message: {
        role: "tool",
        tool_call_id: toolCall.id,
        content: normalizeToolResponse({ content, error }, mode)
      },
      memPatch
    };
  } catch (err) {
    const msg =
      err instanceof Error
        ? err.message
        : "Unknown error during tool execution";
    return {
      message: { role: "tool", tool_call_id: toolCall.id, content: text(msg) }
    };
  }
};

/** Detect conflicts & compose an ordered list of memory patches. */
export const composePatches = <M extends ChatMemory>(
  memory: M,
  patches: (ChatMemoryPatch | undefined)[]
): M => {
  let acc = memory;
  const written = new Set<string>();

  for (const patch of patches) {
    if (!patch) continue;
    const beforeKeys = Object.keys(acc);
    acc = patch(acc);
    for (const k of Object.keys(acc)) {
      if (!beforeKeys.includes(k) || acc[k] !== memory[k]) {
        if (written.has(k))
          throw new Error(`Memory‑patch conflict on key '${k}'.`);
        written.add(k);
      }
    }
  }
  return acc;
};

/** Runtime options accepted by {@link callToolAndAppend}. */
export interface CallToolOptions {
  mode?: "openai" | "mcp";
  logger?: Pick<Console, "log" | "warn" | "error">;
}

/**
 * Detect the last assistant message for tool calls, execute them sequentially,
 * append the resulting tool messages, and merge any memory patches.
 */
export const callToolAndAppend = async <Memory extends ChatMemory>(
  messages: readonly Message[],
  memory: Memory,
  tools: Tools<Memory> = {},
  { mode = "openai", logger = console }: CallToolOptions = {}
): Promise<{ messages: readonly Message[]; memory: Memory }> => {
  if (!messages.length) return { messages, memory };
  const last = messages[messages.length - 1];
  if (!isAssistantToolCall(last)) return { messages, memory };

  // Duplicate‑ID guard.
  const ids = last.tool_calls.map((c) => c.id);
  if (new Set(ids).size !== ids.length)
    logger.error("Duplicate tool_call IDs detected.");

  const results: { message: ToolMessage; memPatch?: ChatMemoryPatch }[] = [];
  for (const call of last.tool_calls) {
    const def = tools[call.function.name];
    if (!def) {
      results.push({
        message: {
          role: "tool",
          tool_call_id: call.id,
          content: text(`No handler for tool \"${call.function.name}\"`)
        }
      });
      continue;
    }
    const { handler, tool } = typeof def === "function" ? await def() : def;
    results.push(
      await executeToolCall(
        call,
        handler,
        tool.function.parameters,
        memory,
        mode
      )
    );
  }

  return {
    messages: [...messages, ...results.map((r) => r.message)],
    memory: composePatches(
      memory,
      results.map((r) => r.memPatch)
    )
  };
};

/** Provider‑agnostic part of a chat‑completion request. */
export interface CompletionRequestBase {
  model: string;
  messages: Message[];
  n?: number;
  stream?: boolean;
  stop?: string | string[];
  max_tokens?: number;
  user?: string;
  seed?: number;
  temperature?: number;
}

/** Extras understood by the OpenAI HTTP endpoint. */
export interface CompletionRequestOpenAI {
  top_p?: number;
  presence_penalty?: number;
  frequency_penalty?: number;
  logit_bias?: Record<string, number>;
  tools?: readonly Tool[];
  tool_choice?:
    | "none"
    | "auto"
    | { type: "function"; function: { name: string } };
}

/** Complete request object accepted by our HTTP wrapper. */
export type CompletionRequest = CompletionRequestBase & CompletionRequestOpenAI;
